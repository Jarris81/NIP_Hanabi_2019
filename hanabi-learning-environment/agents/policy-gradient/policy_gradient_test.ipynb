{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy as sc\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import rl_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(game_type='Hanabi-Full', num_players=2):\n",
    "  \"\"\"Creates the Hanabi environment.\n",
    "\n",
    "  Args:\n",
    "    game_type: Type of game to play. Currently the following are supported:\n",
    "      Hanabi-Full: Regular game.\n",
    "      Hanabi-Small: The small version of Hanabi, with 2 cards and 2 colours.\n",
    "    num_players: Int, number of players to play this game.\n",
    "\n",
    "  Returns:\n",
    "    A Hanabi environment.\n",
    "  \"\"\"\n",
    "  return rl_env.make(\n",
    "      environment_name=game_type, num_players=num_players, pyhanabi_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_model(num_actions, state_shape, num_units=512):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu), # needed -> guess not\n",
    "        #keras.layers.Reshape((1, num_units)),\n",
    "        #keras.layers.LSTM(num_units, return_sequences=True),\n",
    "        #keras.layers.LSTM(num_units),\n",
    "        keras.layers.Dense(num_actions, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_actor_model_NEW(num_actions, state_shape, num_units=512):\n",
    "    x = tf.placeholder(tf.float32, shape=[None,state_shape], name='x')\n",
    "    actions = tf.placeholder(tf.float32, shape=[None,num_actions], name='actions')\n",
    "    \n",
    "    h1 = tf.layers.dense(x, units=num_units, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, units=num_units, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(h2, units=num_actions, activation=None)\n",
    "    y = tf.nn.softmax(logits)\n",
    "    \n",
    "    negative_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits=logits)\n",
    "    \n",
    "    advantage = tf.placeholder(tf.float32, shape=[None,], name='advantage')\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.multiply(negative_log_prob, advantage))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    return y, x, train, loss, advantage, actions\n",
    "\n",
    "def create_critic_model(state_shape, num_units=256):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam',\n",
    "                metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(actor, state, legal_moves):\n",
    "    policy = actor.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_action_NEW(sess, y, x, state, legal_moves):\n",
    "    policy = sess.run(y, {x: [state]}).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_value_estimate(critic, state):\n",
    "    return critic.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCritic(critic, states, Rts, ep_lens):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    \n",
    "    for traj in range(states.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        train_data.extend(states[traj, :H])\n",
    "        train_label.extend(Rts[traj, :H])\n",
    "    \n",
    "    critic.train_on_batch(np.array(train_data), train_label)\n",
    "    \n",
    "def updateCritic_NEW(critic, states, Rts):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    \n",
    "    for traj in range(states.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        train_data.extend(states[traj, :H])\n",
    "        train_label.extend(Rts[traj, :H])\n",
    "    \n",
    "    critic.train_on_batch(np.array(train_data), train_label)\n",
    "    \n",
    "    \n",
    "def update_actor(train, x, states):\n",
    "    sess.run((train, loss), {x: states})\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRtsAndAts(rewards, baselines, ep_lens, gamma=0.95):\n",
    "    Rts = np.zeros(rewards.shape)\n",
    "    Ats = np.zeros(rewards.shape)\n",
    "    \n",
    "    for traj in range(rewards.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        Rt = 0\n",
    "        for t in reversed(range(H)):\n",
    "            Rt += rewards[traj, t]\n",
    "            Rts[traj, t] = Rt\n",
    "            At = Rt -  baselines[traj, t]\n",
    "            Ats[traj, t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "def calculateRtsAndNormalizedAts(rewards, baselines, gamma=0.95):\n",
    "    Rts = []\n",
    "    Ats = []\n",
    "    \n",
    "    for traj in range(len(rewards)):\n",
    "        Rt = 0\n",
    "        T = len(rewards[traj])\n",
    "        Rts.append(np.zeros(T))\n",
    "        Ats.append(np.zeros(T))\n",
    "        for t in reversed(range(T - 1)):\n",
    "            Rt += rewards[traj][t]\n",
    "            Rts[traj][t] = Rt\n",
    "            At = Rt - baselines[traj][t]\n",
    "            Ats[traj][t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "        \n",
    "        #normalize only advantages,since rewards are used for training the critic\n",
    "        Ats[traj] -= Ats[traj].mean()\n",
    "        Ats[traj] /= (Ats[traj].std() + 10**(-10))\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "# Old O(n^2) version\n",
    "def calculateRtsAndAts2(rewards, baselines, ep_lens, gamma=0.95):\n",
    "    Rts = np.zeros(rewards.shape)\n",
    "    Ats = np.zeros(rewards.shape)\n",
    "    \n",
    "    for traj in range(rewards.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        for t in range(H):\n",
    "            Rt = calculateSingleRt(rewards[traj, t:H], gamma)\n",
    "            At = Rt -  baselines[traj, t]\n",
    "            Rts[traj, t] = Rt\n",
    "            Ats[traj, t] = At\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "def calculateSingleRt(rewards, gamma):\n",
    "    rt = 0\n",
    "    for t in range(len(rewards)):\n",
    "        rt += gamma**(t)*rewards[t]\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor = create_actor_model(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "actor.summary()\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 110) (1000,) 21.924912929534912\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 1000\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, _ = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, st] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.278"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(rewards, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100*200\n",
    "num_trajectories = 10\n",
    "t0 = time.time()\n",
    "\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    actions_ = np.zeros((num_trajectories, 110, env.num_moves())) \n",
    "    states = np.zeros((num_trajectories, 110, env.vectorized_observation_shape()[0]))\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    baselines = np.zeros((num_trajectories, 110))\n",
    "    logprobs = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            actions_[traj, st, action] = 1\n",
    "            states[traj, st] = state\n",
    "            logprobs[traj, st] = logprob\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj, st] = get_value_estimate(critic, state)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, st] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    # TODO update critic net and actor net\n",
    "    # TODO normalize Rts and Ats after calculating them or before? \n",
    "    Rts, Ats = calculateRtsAndAts(rewards, baselines, ep_lens)\n",
    "    discounted_Rts, discounted_Ats = (Rts-Rts.mean())/Rts.std(), (Ats-Ats.mean())/Ats.std()\n",
    "    x_train = np.reshape(states, (-1, env.vectorized_observation_shape()[0]))\n",
    "    adv = discounted_Ats.reshape((-1))\n",
    "    acts = actions_.reshape((-1,env.num_moves()))\n",
    "    updateCritic(critic, states, Rts, ep_lens)\n",
    "    sess.run((train, loss), {x: x_train, advantage: adv, actions: acts})\n",
    "    \n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-7a4c18077e7a>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "iteration # 0 , time passed:  0.6199898719787598\n",
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "iteration # 100 , time passed:  488.19078516960144\n",
      "iteration # 200 , time passed:  973.3520650863647\n",
      "iteration # 300 , time passed:  1458.708647966385\n",
      "iteration # 400 , time passed:  1929.6634273529053\n",
      "iteration # 500 , time passed:  2402.241612434387\n",
      "iteration # 600 , time passed:  2871.0337715148926\n",
      "iteration # 700 , time passed:  3346.5941689014435\n",
      "iteration # 800 , time passed:  3824.870575428009\n",
      "iteration # 900 , time passed:  4289.351590633392\n",
      "iteration # 1000 , time passed:  4756.639518737793\n",
      "iteration # 1100 , time passed:  5226.8799700737\n",
      "iteration # 1200 , time passed:  5691.2513427734375\n",
      "iteration # 1300 , time passed:  6159.584987878799\n",
      "iteration # 1400 , time passed:  6630.2483949661255\n",
      "iteration # 1500 , time passed:  7112.777230262756\n",
      "iteration # 1600 , time passed:  7571.471971988678\n",
      "iteration # 1700 , time passed:  8037.6693794727325\n",
      "iteration # 1800 , time passed:  8507.818193435669\n",
      "iteration # 1900 , time passed:  8970.807784080505\n",
      "iteration # 2000 , time passed:  9437.68306350708\n",
      "iteration # 2100 , time passed:  9903.783159732819\n",
      "iteration # 2200 , time passed:  10370.238899230957\n",
      "iteration # 2300 , time passed:  10834.447049379349\n",
      "iteration # 2400 , time passed:  11290.202842473984\n",
      "iteration # 2500 , time passed:  11751.890682220459\n",
      "iteration # 2600 , time passed:  12211.803255796432\n",
      "iteration # 2700 , time passed:  12674.33471775055\n",
      "iteration # 2800 , time passed:  13136.481068849564\n",
      "iteration # 2900 , time passed:  13589.736251115799\n",
      "iteration # 3000 , time passed:  14043.626559972763\n",
      "iteration # 3100 , time passed:  14505.462813615799\n",
      "iteration # 3200 , time passed:  14964.886511325836\n",
      "iteration # 3300 , time passed:  15425.071020841599\n",
      "iteration # 3400 , time passed:  15883.070050239563\n",
      "iteration # 3500 , time passed:  16342.71507525444\n",
      "iteration # 3600 , time passed:  16796.70869255066\n",
      "iteration # 3700 , time passed:  17254.540923833847\n",
      "iteration # 3800 , time passed:  17715.473368883133\n",
      "iteration # 3900 , time passed:  18174.584923505783\n",
      "iteration # 4000 , time passed:  18632.617377519608\n",
      "iteration # 4100 , time passed:  19093.092042207718\n",
      "iteration # 4200 , time passed:  19550.5989921093\n",
      "iteration # 4300 , time passed:  20012.093264341354\n",
      "iteration # 4400 , time passed:  20471.851753473282\n",
      "iteration # 4500 , time passed:  20931.23930454254\n",
      "iteration # 4600 , time passed:  21395.211735248566\n",
      "iteration # 4700 , time passed:  21854.00016283989\n",
      "iteration # 4800 , time passed:  22311.619882822037\n",
      "iteration # 4900 , time passed:  22772.87599682808\n",
      "100 100 5000 23237.019360780716\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 5000\n",
    "num_trajectories = 100\n",
    "t0 = time.time()\n",
    "\n",
    "# create hanabi environment\n",
    "env = create_environment()\n",
    "# init simple keras critic \n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "losses = []\n",
    "for it in range(num_iterations):\n",
    "    if(it % 100 == 0):\n",
    "        tt = time.time()\n",
    "        print('iteration #', it, ', time passed: ', tt-t0)\n",
    "    \n",
    "    actions_ = [] \n",
    "    states = []\n",
    "    rewards = []\n",
    "    baselines = []\n",
    "    logprobs = []\n",
    "    ep_lens = []\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "        actions_.append([])\n",
    "        states.append([])\n",
    "        rewards.append([])\n",
    "        baselines.append([])\n",
    "        logprobs.append([])\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            action_ = np.zeros(env.num_moves())\n",
    "            action_[action] = 1\n",
    "            actions_[traj].append(action_)\n",
    "            states[traj].append(state)\n",
    "            logprobs[traj].append(logprob)\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj].append(get_value_estimate(critic, state))\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj].append(reward)\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens.append(st)\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    Rts, Ats = calculateRtsAndNormalizedAts(rewards, baselines)\n",
    "    # 'flatten' the lists\n",
    "    x_train = np.array(reduce(lambda x,y: np.vstack((x,y)), states))\n",
    "    adv = np.array(reduce(lambda x,y: np.hstack((x,y)), Ats))\n",
    "    rew = np.array(reduce(lambda x,y: np.hstack((x,y)), Rts))\n",
    "    acts = np.array(reduce(lambda x,y: np.vstack((x,y)), actions_))\n",
    "    \n",
    "    # train critic and actor\n",
    "    critic.train_on_batch(x_train, rew)\n",
    "    _, loss_t = sess.run((train, loss), {x: x_train, advantage: adv, actions: acts})\n",
    "    losses.append(loss_t)\n",
    "    \n",
    "    \n",
    "t1 = time.time()\n",
    "print(len(rewards), len(ep_lens), len(losses), t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1968439800.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(map(lambda x: -x,losses)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tensorflow2",
   "language": "python",
   "name": "venv_tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
