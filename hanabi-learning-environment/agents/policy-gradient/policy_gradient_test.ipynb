{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy as sc\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import rl_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(game_type='Hanabi-Full', num_players=2):\n",
    "  \"\"\"Creates the Hanabi environment.\n",
    "\n",
    "  Args:\n",
    "    game_type: Type of game to play. Currently the following are supported:\n",
    "      Hanabi-Full: Regular game.\n",
    "      Hanabi-Small: The small version of Hanabi, with 2 cards and 2 colours.\n",
    "    num_players: Int, number of players to play this game.\n",
    "\n",
    "  Returns:\n",
    "    A Hanabi environment.\n",
    "  \"\"\"\n",
    "  return rl_env.make(\n",
    "      environment_name=game_type, num_players=num_players, pyhanabi_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_model_NEW(num_actions, state_shape, num_units=512):\n",
    "    x = tf.placeholder(tf.float32, shape=[None,state_shape], name='x')\n",
    "    actions = tf.placeholder(tf.float32, shape=[None,num_actions], name='actions')\n",
    "    \n",
    "    h1 = tf.layers.dense(x, units=num_units, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, units=num_units, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(h2, units=num_actions, activation=None, name='logits', reuse=tf.AUTO_REUSE)\n",
    "    y = tf.nn.softmax(logits, name='y')\n",
    "    \n",
    "    negative_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits=logits, name='neg_log_prob')\n",
    "    #negative_log_prob = tf.losses.softmax_cross_entropy(onehot_labels=actions, logits=logits)\n",
    "    #negative_log_prob = -tf.log(y)\n",
    "    \n",
    "    advantage = tf.placeholder(tf.float32, shape=[None,], name='advantage')\n",
    "    \n",
    "    loss = tf.reduce_mean(negative_log_prob*advantage, name='loss')#tf.reduce_mean(tf.multiply(negative_log_prob, advantage))\n",
    "    optimizer = tf.train.AdamOptimizer(0.3)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    return y, x, train, loss, advantage, actions\n",
    "\n",
    "def create_critic_model(state_shape, num_units=1024):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    opt = keras.optimizers.Adam(0.0001)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=opt,\n",
    "                metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(actor, state, legal_moves):\n",
    "    policy = actor.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_action_NEW(sess, y, x, state, legal_moves):\n",
    "    policy = sess.run(y, {x: [state]}).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_value_estimate(critic, state):\n",
    "    return critic.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRtsAndAts(rewards, baselines, gamma=0.95):\n",
    "    Rts = []\n",
    "    Ats = []\n",
    "    \n",
    "    for traj in range(len(rewards)):\n",
    "        Rt = 0\n",
    "        T = len(rewards[traj])\n",
    "        Rts.append(np.zeros(T))\n",
    "        Ats.append(np.zeros(T))\n",
    "        for t in reversed(range(T)):\n",
    "            Rt += rewards[traj][t]\n",
    "            Rts[traj][t] = Rt\n",
    "            At = Rt - baselines[traj][t]\n",
    "            Ats[traj][t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "        \n",
    "    return Rts, Ats\n",
    "\n",
    "# Old O(n^2) version\n",
    "def calculateRtsAndAts2(rewards, baselines, ep_lens, gamma=0.99):\n",
    "    Rts = np.zeros(rewards.shape)\n",
    "    Ats = np.zeros(rewards.shape)\n",
    "    \n",
    "    for traj in range(rewards.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        for t in range(H):\n",
    "            Rt = calculateSingleRt(rewards[traj, t:H], gamma)\n",
    "            At = Rt -  baselines[traj, t]\n",
    "            Rts[traj, t] = Rt\n",
    "            Ats[traj, t] = At\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "def calculateSingleRt(rewards, gamma):\n",
    "    rt = 0\n",
    "    for t in range(len(rewards)):\n",
    "        rt += gamma**(t)*rewards[t]\n",
    "    return rt\n",
    "\n",
    "def calculateQt(critic, r_t, s_t, gamma=0.95):\n",
    "    V_s_t = get_value_estimate(critic, s_t)\n",
    "    \n",
    "    return r_t + gamma * V_s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92dcabbcebc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#actor = create_actor_model(env.num_moves(), env.vectorized_observation_shape()[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_critic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized_observation_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#actor.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#critic.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "#actor = create_actor_model(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "#actor.summary()\n",
    "#critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 1000\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    rewards = np.zeros(num_trajectories)\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, _ = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                # store final reward\n",
    "                rewards[traj] = -reward\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state['legal_moves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-9b7d8b3c6fe8>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "iteration # 0 , time passed:  0.6523292064666748\n",
      "WARNING:tensorflow:From /home/alex/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "20 20 1 3.591775417327881\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 20\n",
    "t0 = time.time()\n",
    "\n",
    "# create hanabi environment\n",
    "env = create_environment()\n",
    "# init simple keras critic \n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "losses = []\n",
    "losses_c = []\n",
    "for it in range(num_iterations):\n",
    "    if(it % 10 == 0):\n",
    "        tt = time.time()\n",
    "        print('iteration #', it, ', time passed: ', tt-t0)\n",
    "    \n",
    "    actions_ = [] \n",
    "    states = []\n",
    "    rewards = []\n",
    "    baselines = []\n",
    "    logprobs = []\n",
    "    ep_lens = []\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "        actions_.append([])\n",
    "        states.append([])\n",
    "        rewards.append([])\n",
    "        baselines.append([])\n",
    "        logprobs.append([])\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            action_ = np.zeros(env.num_moves())\n",
    "            action_[action] = 1\n",
    "            actions_[traj].append(action_)\n",
    "            states[traj].append(state)\n",
    "            logprobs[traj].append(logprob)\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj].append(get_value_estimate(critic, state))\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            #rewards[traj].append(reward)\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens.append(st)\n",
    "                rewards[traj].append(-1*reward)\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "            else:\n",
    "                rewards[traj].append(0)\n",
    "    \n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    Rts, Ats = calculateRtsAndAts(rewards, baselines)\n",
    "    # 'flatten' the lists\n",
    "    x_train = np.array(reduce(lambda x,y: np.vstack((x,y)), states))\n",
    "    adv = np.array(reduce(lambda x,y: np.hstack((x,y)), Ats))\n",
    "    adv -= adv.mean()\n",
    "    adv /= adv.std() + 10**-10\n",
    "    rew = np.array(reduce(lambda x,y: np.hstack((x,y)), Rts))\n",
    "    rew -= rew.mean()\n",
    "    rew /= rew.std() + 10**-10\n",
    "    acts = np.array(reduce(lambda x,y: np.vstack((x,y)), actions_))\n",
    "    \n",
    "    \n",
    "    # train critic and actor\n",
    "    loss_c = critic.train_on_batch(x_train, rew)\n",
    "    _, loss_t = sess.run((train, loss), {x: x_train, advantage: adv, actions: acts})\n",
    "    losses.append(loss_t)\n",
    "    losses_c.append(loss_c)\n",
    "    \n",
    "    \n",
    "saver.save(sess, 'my_test_model')\n",
    "t1 = time.time()\n",
    "print(len(rewards), len(ep_lens), len(losses), t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('my_test_model.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    graph = tf.get_default_graph()\n",
    "    x = graph.get_tensor_by_name('x:0')\n",
    "    y = graph.get_tensor_by_name('y:0')\n",
    "    #init = tf.global_variables_initializer()\n",
    "    #saver = tf.train.Saver()\n",
    "    #sess = tf.Session()\n",
    "    #sess.run(init)\n",
    "    #saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(sess.run(y, {x: [state]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bline = np.array(reduce(lambda r1, r2: np.sum(r1)+np.sum(r2) ,rewards))/ num_trajectories\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_environment()\n",
    "observations = env.reset()\n",
    "state = observations['player_observations'][0]['vectorized']\n",
    "#critic.predict(np.reshape(state, (1,-1))).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, (18,10))\n",
    "#plt.ylim((-100,60000))\n",
    "plt.plot(np.array(list(map(lambda x: (-x[0],-x[1]),losses_c)))[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, (18,10))\n",
    "#plt.ylim((-100,60000))\n",
    "plt.plot(list(map(lambda x: -x,losses)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Rts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tensorflow2",
   "language": "python",
   "name": "venv_tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
