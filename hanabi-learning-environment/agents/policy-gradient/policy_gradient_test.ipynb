{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy as sc\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import rl_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(game_type='Hanabi-Full', num_players=2):\n",
    "  \"\"\"Creates the Hanabi environment.\n",
    "\n",
    "  Args:\n",
    "    game_type: Type of game to play. Currently the following are supported:\n",
    "      Hanabi-Full: Regular game.\n",
    "      Hanabi-Small: The small version of Hanabi, with 2 cards and 2 colours.\n",
    "    num_players: Int, number of players to play this game.\n",
    "\n",
    "  Returns:\n",
    "    A Hanabi environment.\n",
    "  \"\"\"\n",
    "  return rl_env.make(\n",
    "      environment_name=game_type, num_players=num_players, pyhanabi_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_model(num_actions, state_shape, num_units=512):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu), # needed -> guess not\n",
    "        #keras.layers.Reshape((1, num_units)),\n",
    "        #keras.layers.LSTM(num_units, return_sequences=True),\n",
    "        #keras.layers.LSTM(num_units),\n",
    "        keras.layers.Dense(num_actions, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_actor_model_NEW(num_actions, state_shape, num_units=512):\n",
    "    x = tf.placeholder(tf.float32, shape=[None,state_shape], name='x')\n",
    "    actions = tf.placeholder(tf.float32, shape=[None,num_actions], name='actions')\n",
    "    \n",
    "    h1 = tf.layers.dense(x, units=num_units, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, units=num_units, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(h2, units=num_actions, activation=None)\n",
    "    y = tf.nn.softmax(logits)\n",
    "    \n",
    "    negative_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits=logits)\n",
    "    \n",
    "    advantage = tf.placeholder(tf.float32, shape=[None,], name='advantage')\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.multiply(negative_log_prob, advantage))\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    return y, x, train, loss, advantage, actions\n",
    "\n",
    "def create_critic_model(state_shape, num_units=256):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam',\n",
    "                metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(actor, state, legal_moves):\n",
    "    policy = actor.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_action_NEW(sess, y, x, state, legal_moves):\n",
    "    policy = sess.run(y, {x: [state]}).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_value_estimate(critic, state):\n",
    "    return critic.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCritic(critic, states, Rts, ep_lens):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    \n",
    "    for traj in range(states.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        train_data.extend(states[traj, :H])\n",
    "        train_label.extend(Rts[traj, :H])\n",
    "    \n",
    "    critic.train_on_batch(np.array(train_data), train_label)\n",
    "    \n",
    "def updateCritic_NEW(critic, states, Rts):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    \n",
    "    for traj in range(states.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        train_data.extend(states[traj, :H])\n",
    "        train_label.extend(Rts[traj, :H])\n",
    "    \n",
    "    critic.train_on_batch(np.array(train_data), train_label)\n",
    "    \n",
    "    \n",
    "def update_actor(train, x, states):\n",
    "    sess.run((train, loss), {x: states})\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRtsAndAts(rewards, baselines, ep_lens, gamma=0.95):\n",
    "    Rts = np.zeros(rewards.shape)\n",
    "    Ats = np.zeros(rewards.shape)\n",
    "    \n",
    "    for traj in range(rewards.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        Rt = 0\n",
    "        for t in reversed(range(H)):\n",
    "            Rt += rewards[traj, t]\n",
    "            Rts[traj, t] = Rt\n",
    "            At = Rt -  baselines[traj, t]\n",
    "            Ats[traj, t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "def calculateRtsAndNormalizedAts(rewards, baselines, gamma=0.95):\n",
    "    Rts = []\n",
    "    Ats = []\n",
    "    \n",
    "    for traj in range(len(rewards)):\n",
    "        Rt = 0\n",
    "        T = len(rewards[traj])\n",
    "        Rts.append(np.zeros(T))\n",
    "        Ats.append(np.zeros(T))\n",
    "        for t in reversed(range(T - 1)):\n",
    "            Rt += rewards[traj][t]\n",
    "            Rts[traj][t] = Rt\n",
    "            At = Rt - baselines[traj][t]\n",
    "            Ats[traj][t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "        \n",
    "        #normalize only advantages,since rewards are used for training the critic\n",
    "        Ats[traj] -= Ats[traj].mean()\n",
    "        Ats[traj] /= (Ats[traj].std() + 10**(-10))\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "# Old O(n^2) version\n",
    "def calculateRtsAndAts2(rewards, baselines, ep_lens, gamma=0.95):\n",
    "    Rts = np.zeros(rewards.shape)\n",
    "    Ats = np.zeros(rewards.shape)\n",
    "    \n",
    "    for traj in range(rewards.shape[0]):\n",
    "        H = ep_lens[traj]\n",
    "        for t in range(H):\n",
    "            Rt = calculateSingleRt(rewards[traj, t:H], gamma)\n",
    "            At = Rt -  baselines[traj, t]\n",
    "            Rts[traj, t] = Rt\n",
    "            Ats[traj, t] = At\n",
    "            \n",
    "    return Rts, Ats\n",
    "\n",
    "def calculateSingleRt(rewards, gamma):\n",
    "    rt = 0\n",
    "    for t in range(len(rewards)):\n",
    "        rt += gamma**(t)*rewards[t]\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor = create_actor_model(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "actor.summary()\n",
    "critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 1000\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, _ = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, st] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100*200\n",
    "num_trajectories = 10\n",
    "t0 = time.time()\n",
    "\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    actions_ = np.zeros((num_trajectories, 110, env.num_moves())) \n",
    "    states = np.zeros((num_trajectories, 110, env.vectorized_observation_shape()[0]))\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    baselines = np.zeros((num_trajectories, 110))\n",
    "    logprobs = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            actions_[traj, st, action] = 1\n",
    "            states[traj, st] = state\n",
    "            logprobs[traj, st] = logprob\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj, st] = get_value_estimate(critic, state)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, st] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    # TODO update critic net and actor net\n",
    "    # TODO normalize Rts and Ats after calculating them or before? \n",
    "    Rts, Ats = calculateRtsAndAts(rewards, baselines, ep_lens)\n",
    "    discounted_Rts, discounted_Ats = (Rts-Rts.mean())/Rts.std(), (Ats-Ats.mean())/Ats.std()\n",
    "    x_train = np.reshape(states, (-1, env.vectorized_observation_shape()[0]))\n",
    "    adv = discounted_Ats.reshape((-1))\n",
    "    acts = actions_.reshape((-1,env.num_moves()))\n",
    "    updateCritic(critic, states, Rts, ep_lens)\n",
    "    sess.run((train, loss), {x: x_train, advantage: adv, actions: acts})\n",
    "    \n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100\n",
    "num_trajectories = 100\n",
    "t0 = time.time()\n",
    "\n",
    "# create hanabi environment\n",
    "env = create_environment()\n",
    "# init simple keras critic \n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_model_NEW(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "losses = []\n",
    "for it in range(num_iterations):\n",
    "    actions_ = [] \n",
    "    states = []\n",
    "    rewards = []\n",
    "    baselines = []\n",
    "    logprobs = []\n",
    "    ep_lens = []\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "        actions_.append([])\n",
    "        states.append([])\n",
    "        rewards.append([])\n",
    "        baselines.append([])\n",
    "        logprobs.append([])\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            action_ = np.zeros(env.num_moves())\n",
    "            action_[action] = 1\n",
    "            actions_[traj].append(action_)\n",
    "            states[traj].append(state)\n",
    "            logprobs[traj].append(logprob)\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj].append(get_value_estimate(critic, state))\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj].append(reward)\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens.append(st)\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    Rts, Ats = calculateRtsAndNormalizedAts(rewards, baselines)\n",
    "    # 'flatten' the lists\n",
    "    x_train = np.array(reduce(lambda x,y: np.vstack((x,y)), states))\n",
    "    adv = np.array(reduce(lambda x,y: np.hstack((x,y)), Ats))\n",
    "    rew = np.array(reduce(lambda x,y: np.hstack((x,y)), Rts))\n",
    "    acts = np.array(reduce(lambda x,y: np.vstack((x,y)), actions_))\n",
    "    \n",
    "    # train critic and actor\n",
    "    critic.train_on_batch(x_train, rew)\n",
    "    _, loss_t = sess.run((train, loss), {x: x_train, advantage: adv, actions: acts})\n",
    "    losses.append(loss_t)\n",
    "    \n",
    "    \n",
    "t1 = time.time()\n",
    "print(len(rewards), len(ep_lens), len(losses), t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(map(lambda x: -x,losses)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tensorflow2",
   "language": "python",
   "name": "venv_tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
