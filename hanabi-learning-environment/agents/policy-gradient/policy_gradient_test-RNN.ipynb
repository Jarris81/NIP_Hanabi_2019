{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy as sc\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import rl_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(game_type='Hanabi-Full', num_players=2):\n",
    "  \"\"\"Creates the Hanabi environment.\n",
    "\n",
    "  Args:\n",
    "    game_type: Type of game to play. Currently the following are supported:\n",
    "      Hanabi-Full: Regular game.\n",
    "      Hanabi-Small: The small version of Hanabi, with 2 cards and 2 colours.\n",
    "    num_players: Int, number of players to play this game.\n",
    "\n",
    "  Returns:\n",
    "    A Hanabi environment.\n",
    "  \"\"\"\n",
    "  return rl_env.make(\n",
    "      environment_name=game_type, num_players=num_players, pyhanabi_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_lstm(num_actions, state_shape, num_steps, num_trajectories, num_layers=2, num_units=512):\n",
    "    cells = [tf.keras.layers.LSTMCell(num_units), tf.keras.layers.LSTMCell(num_units)]\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=(num_trajectories, num_steps, state_shape), name='x')\n",
    "    actions = tf.placeholder(tf.float32, shape=[None,num_actions], name='actions')\n",
    "    outputs = keras.layers.RNN(cells, return_sequences=True)(x)\n",
    "    \n",
    "    logits = tf.layers.dense(outputs, units=num_actions, activation=None)\n",
    "    y = tf.nn.softmax(logits)\n",
    "    \n",
    "    negative_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions, logits=logits)\n",
    "    \n",
    "    advantage = tf.placeholder(tf.float32, shape=[None,], name='advantage')\n",
    "    \n",
    "    loss = tf.reduce_mean(negative_log_prob*advantage)#tf.reduce_mean(tf.multiply(negative_log_prob, advantage))\n",
    "    optimizer = tf.train.AdamOptimizer(0.1)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    return y, x, train, loss, advantage, actions, negative_log_prob, outputs, logits\n",
    "\n",
    "def create_critic_model(state_shape, num_units=512):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(num_units, input_dim=state_shape, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_units, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam',\n",
    "                metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnn_8/transpose_1:0' shape=(10, 5, 512) dtype=float32>,\n",
       " <tf.Tensor 'dense_7/BiasAdd:0' shape=(10, 5, 3) dtype=float32>,\n",
       " <tf.Tensor 'softmax_cross_entropy_with_logits_6/Reshape_2:0' shape=(10, 5) dtype=float32>,\n",
       " <tf.Tensor 'Softmax_6:0' shape=(10, 5, 3) dtype=float32>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x, train, loss, advantage, actions, negative_log_prob, outputs, logits = create_actor_lstm(3, 7, 5, 10)\n",
    "outputs, logits, negative_log_prob, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 7)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.random.choice([0,1,2], (1,5,7))\n",
    "#xx = xx.reshape((10,5,-1,7))\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 5, 7) for Tensor 'x_8:0', which has shape '(10, 5, 7)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-0f55f6d043e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1128\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1129\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 5, 7) for Tensor 'x_8:0', which has shape '(10, 5, 7)'"
     ]
    }
   ],
   "source": [
    "sess.run(y, {x: xx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(sess, y, x, state, legal_moves):\n",
    "    policy = sess.run(y, {x: [state]}).squeeze()\n",
    "    # somehow only consider the legal actions\n",
    "    policy_legal = np.full(policy.shape, -np.inf)\n",
    "    policy_legal[legal_moves] = policy[legal_moves]\n",
    "    policy_legal = sc.special.softmax(policy_legal)\n",
    "    \n",
    "    action = np.random.choice(policy_legal.shape[0], p=policy_legal)\n",
    "    logprob = np.log(policy_legal[action])\n",
    "    \n",
    "    return action, logprob\n",
    "\n",
    "def get_value_estimate(critic, state):\n",
    "    return critic.predict(np.reshape(state, (1,-1))).squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRtsAndAts(rewards, baselines, gamma=0.95):\n",
    "    Rts = []\n",
    "    Ats = []\n",
    "    \n",
    "    for traj in range(len(rewards)):\n",
    "        Rt = 0\n",
    "        T = len(rewards[traj])\n",
    "        Rts.append(np.zeros(T))\n",
    "        Ats.append(np.zeros(T))\n",
    "        for t in reversed(range(T)):\n",
    "            Rt += rewards[traj][t]\n",
    "            Rts[traj][t] = Rt\n",
    "            At = Rt - baselines[traj][t]\n",
    "            Ats[traj][t] = At\n",
    "            \n",
    "            Rt *= gamma\n",
    "            \n",
    "    return Rts, Ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#actor = create_actor_model(env.num_moves(), env.vectorized_observation_shape()[0])\n",
    "#critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "#rnn = create_actor_lstm(0, 658, 10, 50)\n",
    "#actor.summary()\n",
    "#critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 1000\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories).astype(int)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "\n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, _ = get_action_NEW(sess, y, x, state, moves)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, st] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    \n",
    "t1 = time.time()\n",
    "print(rewards.shape, ep_lens.shape, t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(rewards, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1\n",
    "num_trajectories = 50\n",
    "num_steps = 5\n",
    "t0 = time.time()\n",
    "\n",
    "# create hanabi environment\n",
    "env = create_environment()\n",
    "# init simple keras critic \n",
    "critic = create_critic_model(env.vectorized_observation_shape()[0])\n",
    "# init actor neural net (a custom tf graph)\n",
    "y, x, train, loss, advantage, actions = create_actor_lstm(env.num_moves(), env.vectorized_observation_shape()[0], \n",
    "                                                          num_steps, num_trajectories)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "losses = []\n",
    "for it in range(num_iterations):\n",
    "    if(it % 50 == 0):\n",
    "        tt = time.time()\n",
    "        print('iteration #', it, ', time passed: ', tt-t0)\n",
    "    \n",
    "    actions_ = np.zeros((num_trajectories, 110, env.num_moves())) \n",
    "    states = np.zeros((num_trajectories, 110, env.vectorized_observation_shape()[0]))\n",
    "    rewards = np.zeros((num_trajectories, 110))\n",
    "    baselines = np.zeros((num_trajectories, 110))\n",
    "    logprobs = np.zeros((num_trajectories, 110))\n",
    "    ep_lens = np.zeros(num_trajectories)\n",
    "\n",
    "\n",
    "    # collect trajectories\n",
    "    for traj in range(num_trajectories):\n",
    "        # reset environment\n",
    "        observations = env.reset()\n",
    "        reward = 0\n",
    "        is_done = False\n",
    "                       \n",
    "        # collect one trajectory\n",
    "        for st in range(110):\n",
    "            # extract legal moves and state from observations\n",
    "            moves = observations['player_observations'][st % 2]['legal_moves_as_int']\n",
    "            state = observations['player_observations'][st % 2]['vectorized']\n",
    "\n",
    "            # get next action\n",
    "            action, logprob = get_action(sess, y, x, state, moves)\n",
    "\n",
    "            # store variables\n",
    "            action_[traj, step, action] = 1\n",
    "            states[traj, step] = state)\n",
    "            logprobs[traj, step] = logprob\n",
    "\n",
    "            # get baseline(value estimate)\n",
    "            baselines[traj, step] = get_value_estimate(critic, state)\n",
    "\n",
    "            # do next step\n",
    "            observations, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # store reward after executing action\n",
    "            rewards[traj, step] = reward\n",
    "\n",
    "\n",
    "            if (is_done or st == 110-1):\n",
    "                ep_lens[traj] = st\n",
    "                #print(\"Done\", ep, st)\n",
    "                #print(observations['player_observations'][0]['pyhanabi'])\n",
    "                #print('reward', reward)\n",
    "                #print(' --- ')\n",
    "                break\n",
    "\n",
    "    # collected a bunch of trajectories, now calculate discounted rewards (Rts)\n",
    "    Rts, Ats = calculateRtsAndAts(rewards, baselines)\n",
    "    # 'flatten' the lists\n",
    "    #x_train = np.array(reduce(lambda x,y: np.vstack((x,y)), states))\n",
    "    #adv = np.array(reduce(lambda x,y: np.hstack((x,y)), Ats))\n",
    "    #adv -= adv.mean()\n",
    "    #adv /= adv.std() + 10**-10\n",
    "    rew = np.array(reduce(lambda x,y: np.hstack((x,y)), Rts))\n",
    "    #acts = np.array(reduce(lambda x,y: np.vstack((x,y)), actions_))\n",
    "    \n",
    "    # train critic and actor\n",
    "    \n",
    "    #critic.train_on_batch(x_train, rew)\n",
    "    rew -= rew.mean()\n",
    "    rew /= rew.std() + 10**-10\n",
    "    \n",
    "    max_ep_len = np.max(ep_lens)\n",
    "    num_batches = np.ceil(max_ep_len / num_steps).astype(int)\n",
    "    for b in range(num_batches):\n",
    "        start = b * num_steps\n",
    "        end = start + num_steps\n",
    "        \n",
    "        x_train = states[:, start:end]\n",
    "        rew = Rts[:, start:end]\n",
    "        acts = actions[:, start:end]\n",
    "        \n",
    "    \n",
    "    \n",
    "    _, loss_t = sess.run((train, loss), {x: x, advantage: rew, actions: acts})\n",
    "    losses.append(loss_t)\n",
    "    \n",
    "    \n",
    "t1 = time.time()\n",
    "print(len(rewards), len(ep_lens), len(losses), t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.predict(np.reshape(state, (1,-1))).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()\n",
    "state = observations['player_observations'][0]['vectorized']\n",
    "critic.predict(np.reshape(state, (1,-1))).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, (18,10))\n",
    "plt.plot(list(map(lambda x: -x,losses)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, (18,10))\n",
    "#plt.ylim((-100,60000))\n",
    "plt.plot(list(map(lambda x: -x,losses)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =\n",
    "start = b * 3\n",
    "end = start + 3\n",
    "a = np.arange(11)[start:end]\n",
    "if (len(a) < 3):\n",
    "    a = np.concatenate((a, np.zeros(3-len(a))))\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tensorflow2",
   "language": "python",
   "name": "venv_tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
